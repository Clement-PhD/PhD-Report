


# Todo

- [ ] make a nixos java env

- [ ] Repo gitlab ?
- [ ] See hyperAst and understand how it works (make it work)
- [ ] read Model-based security vulnerabilities paper
- [ ] have access to kackstabber knife collection [here](https://dasfreak.github.io/Backstabbers-Knife-Collection/)
- [ ] explore research question (rq): Joern/tree-sitter > compare perf/time limit with hyperAst, Maybe make small experiment with one known CVE -> try to detect it inside the repo
- [ ] explore rq : attack on supply chain : analyse of the dependance of open source project > Idea of experiments : extract package name + version from mitre, store this in a structure optimized for search (hashmap + memory ?), use the tool from romain to extract the file related to the packet manager, compare all the dependencies with the list. Challenge : extraction of CVE can be tricky, analyse of first dependencies are easy, but the link with the dependencie of the dependencie no.

- [ ] find and test google tool to search dependencies of dependencies
- [ ] [backstabber's knife collection](https://dasfreak.github.io/Backstabbers-Knife-Collection/) (CVE dataset), but need mail to have accesss
- [ ] try following [mthod](https://github.com/fabric8-analytics/fabric8-analytics-vscode-extension) to have dependencies
- [ ] try [snyk](https://security.snyk.io/vuln) and [retire.js](https://github.com/RetireJS/retire.js) (Aman Anupam et al., “Analysis of Open Source Node.Js Vulnerability Scanners,” IRJET 07, no. 05 (May 2020).)
- [ ] try [google tool](https://cloud.google.com/blog/topics/developers-practitioners/using-the-open-source-insights-dataset?hl=en)

## Software heritage TODO


- [ ] Try to get all the file with name "requirements.txt" (pip dependancies) and compare time execution with number of node of the swh full instance
- [ ] Have access to the online graph api (need a special account) and then finalise the experiences (time to count and extract all snapshots nodes, IE, repo)


- [ ] make a working instance of local software heritage (first for playground, second for experimentation before runing on the real one ?) > suspend now, exploration on the graph
- [ ] check pagination of the endpoint origin (all page are differents ?)
- [ ] check metadata (result of visit enpoint) (All are empty ?)




# Goal

- [ ] 22-12-23 > Slide with 2/3 research questions and exemple of experiments
- [ ] in 6 month (05-24), have the research questions and the methdologie

# questions